# cousera机器学习笔记概要中文翻译
文中省略举例，保留理论精华
## 第一周
### 何为机器学习
两种定义： 
“关于赋予计算机一种不需要显式编码的学习能力的研究领域”——来自Arthur Samuel的旧定义
“一个计算机程序可以从一些类别任务T和表现衡量P相关的经验E中进行学习，如果它在任务中的表现T，由P来衡量，随着经验E而完善。”——来自Tom Mitchell的更现代的定义
例子：玩西洋棋
E = 玩很多西洋棋的经验
T = 玩西洋棋的任务
P = 程序赢得下一盘游戏的概率

通常来说，任何机器学习问题都可以分为两大类：监督学习和无监督学习

#### 监督学习
在监督学习中我们拥有一个数据集并已经知道正确的输出是怎样的，知道输入和输出之间有关系。
监督学习可以分为“回归”和“分类”问题。在回归问题中，我们尝试在连续输出中预测结果，意味着我们把输入变量映射到一些连续函数。在分类问题中，我们尝试以离散输出形式预测结果，把输入变量映射到离散类别中。

#### 无监督学习
无监督学习允许我们解决不知道或者很少知道结果是怎样的问题。我们可以从数据中获得结构，在这些数据中我们不必知道变量的效果。
我们可以根据数据中变量的关系对数据进行聚集，来获得这个结构。
无监督学习没有根据预测结果的反馈，即没有老师来更正你。
分为聚类问题和非聚类问题（鸡尾酒派对问题）

### 单变量线性回归
#### 模型表现
在回归问题中，我们用输入变量把输出值拟合到一个连续函数上。
单变量线性回归也叫“通用线性回归”。
当你想从一个单输入变量X预测一个单输出变量y的时候，通用线性回归就得到使用。
我们这时在做监督学习，所以这意味着我们已经知道输入输出的原因和效果应该是怎样。

####  假设函数
我们的假设函数有以下一般形式：$$y = h_{\theta}(x) = \theta_0 + \theta_1 x$$
注意到这跟直线方程很相似。我们给$h_\theta(x)$以$\theta_0$和$\theta_1$值来得到我们的估计 输出$y$。换句话说，我们尝试创造一个叫$h_\theta$的函数来把输入数据（X）映射到输出数据（y）。

#### 损失函数
我们可以用一个损失函数来衡量假设函数的精准性。
这采用所有从X的输入得到的假设结果与实际的输出y之间比较的平均值（实际上是一种更新奇的平均数版本）。
$$J(\theta_0,\theta_1)=\frac{1}{2m}\sum_{i=1}^m(h_\theta(x_i)-y_i)^2$$
分开来讲，$\frac{1}{2}x$，当$x$是$h_\theta(x_i)-y$的平方的均值，或者说预测值与实际值的差异。
这个函数也叫做“平方误差函数”(square error function)，或者“均值平方误差”(mean squared error)。该均值被对半分（$\frac{1}{2m}$），为了方便计算梯度下降，因为平方函数的导数项会把$\frac{1}{2}$消掉。
现在我们能够精确地衡量预测函数关于我们拥有的正确值的精确度，于是我们可以预测我们没有的新结果。

如果我们尝试从视觉角度来看，我们的训练数据集分散在x-y平面上。我们正尝试画一条直线（由$h_\theta(x)$定义），它穿过这个分散的数据集。我们的目标是得到最合适的直线。最合适的直线将会使得分散的点到直线的垂直距离的平方的的均值最小。在最好的情况下，这条线会穿过所有训练集中的点。在这中情况下$J(\theta_0,\theta_1)$的值是0。

### 梯度下降
现在我们有了假设函数和衡量它能多好地拟合数据的方法。现在我们需要估计假设函数中的参数。
这就是梯度下降的用武之处。
假想我们根据$假设函数的域\theta_0$和$\theta_1$画出来假设函数（实际上我们画出损失函数作为参数估计的函数）。这有点令人疑惑；我们开始更高层的抽象。我们不是在画x和y自身，而是假设函数的参数范围和选择特定参数集带来的损失。

我们把$\theta_0$放在x轴上，$\theta_1$放在y轴上，损失函数放在垂直的z轴上。图像上的点将会是随着那些特定的theta参数使用我们的假设得到的损失函数的结果。

我们都知道当我们的损失函数在图像的凹点的底部时我们就成功了，即当它的值最小时。

做到这个的方法是通过计算损失函数的导数（函数的切线）。切线的斜率就是那个点的导数，并给予我们移动的方向。我们沿着梯度最陡峭的方向一步步走下损失函数，每步 的大小由一个参数$\alpha$决定，叫做学习率。

梯度下降算法是：
重复直到收敛：
$$\theta_j:=\theta_j-\alpha\frac{\alpha}{\alpha\theta_j}J(\theta_0,\theta_1)$$
这里
j=0,1表示特征索引数字
直观地，这可以认为是
重复直到收敛：
$$\theta_j:=\theta_j-\alpha[j维度的导数][i维度的导数]$$

#### 线性回归中的梯度下降
当应用到线性回归的情况时，一种新的梯度下降等式就衍生出来了。我们代替实际的损失函数和实际的假设函数，并把等式修改为
$$重复直到收敛:{\\
\theta_0:=\theta_0-\alpha\frac{1}{m}\sum_{i=1}^m(h_\theta(x_i)-y_i)\\
\theta_1:=\theta_1-\alpha\frac{1}{m}\sum_{i=1}^m((h_\theta(x_i)-y_i)x_i)\\
}$$

这里m是训练集的大小，$\theta_0$是随着$\theta_1$同步改变的常量，$x_i,y_i$是给定训练集的值。
注意到我们把两个关于$\theta_j$的情况分开成单独的关于$\theta_0$和$\theta_1$的等式；针对$\theta_1$由于导数我们在末尾乘了个$x_i$。

这里的要点是如果我们用一个假设而且随后不断重复应用这些梯度下降算法，我们的假设会变得越来越精确。

### 线性代数要点
#### 记号和术语

- $A_{ij}$指的是矩阵A的i行j列的元素
- 一个有n行的向量称为n维向量
- $v_i$指的是向量第i行的元素
- 一般地，我们所有的矩阵和向量都是从1开始编号的。注意到有些编程语言的数组是从0开始编号的。
- 矩阵用大写字母表示，向量用小写字母表示
- 标量(scale)意味着一个对象是一个单独的值，不是向量或矩阵
- $R$指实数集
- $R^n$指n维实数向量的集合（编者按：准确来讲是向量空间）

#### 加法和标量乘法
加法和标量乘法都是针对元素的,所以你可以简单地加或减每个对应元素。
加或减的两个矩阵的维度一定相等。
在标量乘法中，我们简单地把每个元素乘以标量值。

#### 矩阵-向量 乘法
我们把向量的每一列映射到矩阵的每一行上，乘以每一个元素并把结果求和。
一个m*n矩阵乘一个n*1向量的结果是m*1向量。

#### 矩阵-矩阵 乘法
两个矩阵相乘是通过把它分成几个向量乘法并把结果连在一起。
一个m*n矩阵乘一个n*o矩阵的结果是m*o矩阵。
两个矩阵相乘，第一个矩阵的列数等于第二个矩阵的行数。

####矩阵乘法的性质

－不满足交换律
－满足结合律
－单位阵是什么（省略）

####逆阵和转置
一个方阵和逆阵相乘等于单位阵。
非方阵没有逆阵。
没有逆阵的矩阵叫奇异或退化(singular or degenerate)。
矩阵的转置就像把矩阵顺时针转九十度然后翻转。

## 第二周
### 多元线性回归
定义以下记号：
$$x_j^(i): 第i个训练例子的第j个特征的值
x^(i): 第i个训练例子的所有特征列向量
m: 训练例子个数
n: 特征数$$
向量形式$h_{\theta}(x) = \theta^{T}x$
一般来说，$X$以行向量形式储存每个训练例子，$\theta$是列向量，那么
$$h_{\theta}(X) = X\theta$$

### 损失函数
$\theta$是$R^{n+1}$的向量
$$J(\theta) = {1 \over 2m} \sum_{i=1}^m(h_{\theta}(x^{(i)}-y^{(i)})^{2}$$
向量版本是
$$J(\theta) = {1 \over 2m} (X\theta - y)^T(X\theta - y)$$

### 多元梯度下降
$$ 
重复直到收敛:{ \\
    for j:= 0...n \\ 
    \theta_{j} := \theta_{j} - \alpha{1 \over m}\sum_{i=1}^{m}(h_{\theta}(x^{(i)}-y^{(i)})x_{j}^{(i)}
}$$

向量形式：
$$\theta := \theta - \alpha \nabla J(\theta) \\
= \theta - {\alpha \over m} X^T(X\theta - y)$$

### 特征正则化
- 特征放缩：输入值除以范围（最大值与最小值的差）
- 均值归一：输入值减去均值
两个方法一起用

### 梯度下降tips
- 画图看损失函数，如果有上升说明学习率太大
- 设一个足够小的判定收敛的阀值

### 特征与多项式回归
改善模型，考虑把多个特征合成一个
#### 多项式回归
通过对特征取幂形成新的特征，得到多项式假设函数
或者取个平方根
使用这些方法时，特征放缩非常重要

### 标准方程
是一种不用迭代寻找最优值的方法
$$\theta = (X^TX)^{-1}X^Ty$$
推导：令误差的平方的一次导数为零
注意：
- 不用特征放缩
- 复杂度是立方，比梯度下降的平方慢
- $X^TX$有可能不可逆，原因是特征之间线性相关或者特征太多了

## 第三周
逻辑斯特回归
虽有回归之名，其实解决的是分类问题
### 二元分类
0为负类，1为正类
当假设满足$$0<= h_\theta(x) <= 1$$
使用Sigmoid函数，也叫逻辑斯特函数：
$$g(x) = {1\over 1+e^{-z}}\\
let z = \theta^Tx \\ 
h_\theta(x) = g(z) = g(\theta^Tx) $$
输出结果为1的概率

### 决策边界
为了得到二元分类，我们约定：
逻辑斯特函数输出大于等于0.5，表示$y = 1$
逻辑斯特函数输出小于0.5，表示$y = 0$
记住逻辑斯特的函数图像
决策边界就是y=0和y=1的分界线

### 损失函数
原来的用不了了，原来那个已经不是凸函数了
$$J(\theta) = {1 \over m}\sum_{i=1}^m-log[y(1-h_\theta(x)) + (1-y)h_\theta(x)]$$

向量形式：
$$ h = g(X\theta) \\
J(\theta) = {1 \over m}(-y^Tlog(h)-(1-y)^Tlog(1-h))$$

### 梯度下降
与之前的没区别
向量形式多套一个$g(\theta X)$而已

### 损失函数$J(\theta)$的偏导数推导
先算逻辑斯特函数的导数：
$g(z)^{'} = ... = g(z)(1-g(z))$
再对损失函数关于$\theta_j$求偏导：
和式的偏导等于偏导的和式，找到变量$\theta$所在的项，复合函数求导，
注意到${\partial \over \partial \theta_j} \theta^T x^{(i)} = x_j^{(i)}$
(x的各个分量的线性组合对第j个分量求导的结果)
分子分母约一下，合并一下，得到
$${\partial \over \partial \theta_j} J(\theta) = {1\over m}\sum_{i=1}^m[h_{\theta}(x^{(i)})-y^{(i)}]x_j^{(i)}$$

向量形式：
$$\nabla J(\theta) = {1\over m}X^T(g(X\theta)-y)$$

### 高级优化
更好的优化手段：共轭梯度、BFGS、L-BFGS

- BFGS算法（以四个人的名字首字母命名）：
对二阶优化方法————牛顿算法的一种改进，因为牛顿算法考虑二阶导数，而计算Hassian矩阵很耗时，BFGS提出一种方法近似求得Hassian矩阵。
http://www.cnblogs.com/kemaswill/p/3352898.html

- L-BFGS算法：
当Hassian矩阵比较大的时候，用Limited-memory BFGS
https://en.wikipedia.org/wiki/Limited-memory_BFGS

### 多类别分类：一对多
目标是把数据分成n+1类(y={0,1...n})
把问题分割成n+1个二元分类问题
在每个问题中，计算y属于第k个类的概率$P(y=k|x;\theta)$
然后求概率最大值

选择一个类，然后把其他剩下的类看作一个整体，反复运用二元逻辑斯特回归，然后求概率最大值

### 正则化
#### 过拟合
正则化用来解决过拟合问题

高偏差(high bias)/欠拟合：假设函数不能很好地预测数据的趋势。原因是函数太简单或者特征太少
高方差(high variance)/过拟合：能够拟合已有数据的假设函数没有很好地泛化来预测新数据。原因是函数太复杂，产生太多与数据无关的曲线和角

解决过拟合的手段：

- 减少特征的数量： 
1. 手工选择保留哪个特征
2. 使用模型选择(model selection)算法

- 正则化
保留所有特征，但减小参数$\theta$
当我们有很多稍微有用(slightly useful)的特征时，正则化有效。

#### 损失函数
通过增加损失来减小假设函数中某些项的权重($\theta$值)
例如：如果我想让一个四阶函数更像平方函数，那么就要减小三阶和四阶变量的影响（而不是把它们从假设函数中移除），我们可以在损失函数中加上三阶和四阶变量对应的$\theta$参数，并赋予很大的权值。如果想要损失函数接近零，那么就要使得三阶和四阶变量对应的$\theta$参数接近零，从而达到减小三阶和四阶变量影响的目的。

也可以通过求和来正则化所有的$\theta$参数
$$min_{\theta} {1 \over 2m} \sum_{i=1}^m(h_{\theta}(x^{(i)}-y^{(i)})^{2} + \lambda \sum_{j=1}^n \theta_j^2$$
其中$\lambda$是正则化参数，决定$\theta$参数的损失（或惩罚）应该“放大”多少。

使用带有求和项的损失憾事，我们可以把假设函数的输出光滑化(smooth)，从而减小过拟合。但是如果正则化参数太大，会导致过于平滑而欠拟合。

#### 正则化线性回归

- 梯度下降
$$ 
重复直到收敛:{ \\
    \theta_{0} := \theta_{0} - \alpha[{1 \over m}\sum_{i=1}^{m}(h_{\theta}(x^{(i)}-y^{(i)})x_{0}^{(i)}] \\
for j:= 1...n \\ 
\theta_{j} := \theta_{j} - \alpha[{1 \over m}\sum_{i=1}^{m}(h_{\theta}(x^{(i)}-y^{(i)})x_{j}^{(i)} + {\lambda \over m} \theta_j]
}$$

把$\theta_0$跟其他参数分开，因为没必要惩罚它（它不控制x分量）

- 标准方程
$$\theta = (X^TX + \lambda L)^{-1}X^T y$$
$$L = \begin{bmatrix} 0 \\ & 1 \\ &　& 1 \\ & & & \ddots \\ & & & & 1
\end{bmatrix}_{(n+1)\times(n+1)}$$
$\lambda$是正则化参数
保证可逆

#### 正则化逻辑回归

- 损失函数
$$J(\theta) = {1 \over m}\sum_{i=1}^m-log[y(1-h_\theta(x)) + (1-y)h_\theta(x)] + {\lambda \over 2m}\sum_{j=1}^n \theta_j^2$$
明确排除偏项(bias term) $\theta_0$
平方保证是非负

- 梯度下降
跟上文一样

### 初始全一特征向量
#### 常量特征
在训练开始之前，加一个常量特征到特征集中是很重要的。这个特征一般是一个全1的集合。
例如，当特征矩阵是X时，$X_0$就是全一向量。

解释：

1. 电子工程的角度：交流电和直流电
非常量特征捕捉模型中的动态特征，由于输入改变导致的输出变化，相当于DC交流电。
常量特征代表不变部分，就像交流电的直流部分。
而对时序信号做微分就能清除模型中的静态部分。
2. 几何角度
不是所有直线都会经过原点，因此y=ax+b中的b有存在的必要
3. 实际情况
有常量特征和没有常量特征的训练结果相差一个常数。
例如用两套模型（一套有常量特征，一套没有）预测房子A和房子B的价格，两套模型的预测结果可能完全不一样，但是房子A和B的价格差值是一样的。
偏项是当输入全为零时的稳态，代表模型固有的偏差，其他特征产生张力来移动这个偏差位置。

## 第四周
神经网络之表示
### 非线性假设
线性回归很少见
有若干个变量，两两相乘组成一个多项式，配以相关系数，加上常数项，外面套一个函数，形成假设函数。
如果变量一多，这种方法会产生大量特征，不实际。

### 神经元和大脑
神经网络是对我们大脑工作原理的有限模拟。它们最近经历了一个很大的复兴，因为计算机硬件的发展。
大脑只用一种学习算法来完成不同函数。这个准则就是“神经可塑性”。

### 模型表示

- 输入层
- 隐藏层
- 输出层
- 偏移单元（bias unit）
- 激活函数
- 激活单元：隐藏层节点
$$a_i^{(j)} = 第j层第i个节点的激活(activation) \\
\theta^{(j)} = 控制函数从第j层映射到第j+1层的权重矩阵\\
\theta_{ik}^{(j)} = 第j层第k个特征映射到第j+1层第i个激活点的权值$$

权重矩阵的维度是$d_{j+1}\times (d_j+1)$
其中+1来自偏移单元$\theta_j^{0}$和$x_0$。
输入包含偏移单元，而输出不包含。

### 向量表示
令输入层n+1维向量$x = a^{(1)}$，那么
$$z^{(j)} = \theta^{(j-1)}a^{(j-1)}$$
其中$\theta^{(j-1)}$维度是$n\times (n+1)$，z^{(j)}是n维向量
然后$$a^{(j)} = g(z^{(j)}) $$
其中g作用于每个元素
最后增加一个$a_0^{(j)}=1$到$a^{(j)}$当中，继续重复计算
直到最后一层，输出一个值，情况跟逻辑斯特回归一样。

### 多元分类
输出层是一个由0和1组成的向量，1表示属于该分类。

## 第五周
神经网络之学习
### 损失函数
定义：
$L$是网络总层数
$s_l$是第l层除去偏移单元之外的节点数
$K$是输出节点数，即分类数
$h_{\theta}(x)_k$是产生第k个输出的假设

假设函数：
$$J(\theta) = -{1\over m}\sum_{i=1}^m\sum_{k=1}^K[y_k^{(i)}log(h_{\theta}(x^{(i)})_k) + (1-y_k^{(i)})log(1-h_{\theta}(x^{(i)})_k)] + {\lambda \over 2m}\sum_{l=1}^{L-1}\sum_{i=1}^{s_l}\sum_{j=1}^{s_l+1}(\theta_{j,i}^{(l)})^2$$

对所有样本的所有输出节点求和。
正则化项对把所有参数平方求和。

### Backpropagation算法
用来最小化损失函数的算法
给定训练集${(x^{(1)},y^{(1)}...(x^{(m)},y^{(m)})}$

- 设定$\Delta_{i,j}^{(l)}$初始值为0
对于从1到m的第t个训练样本：

- 令$a^{(1)}:=x^{(t)}$
- 执行forward propagation计算$a^{(l)}$（l=2,3...L）
- 计算$\delta^{(L)}=a^{(L)}-y^{(T)}$
- 使用$\delta^{(L)}=((\theta^{(l)})^T\delta^{(l+1)}).*a^{(l)}.*(1-a^{(l)})$计算$\delta^{(L-1)},\delta^{(L-2)}...\delta^{(2)}$
- $\Delta_{i,j}^{(l)}:=\Delta_{i,j}^{(l)}+a_j^{(l)}\delta_i^{(l+1)}$ 或者向量形式$\Delta^{(l)}:=\Delta^{(l)}+\delta^{(l+1)}(a^{(l)})^T$
- $D_{i,j}^{(l)}:={1\over m}(\delta_{i,j}^{(l)}+\lambda\theta_{i,j}^{(l)})$ if j!=0
- $D_{i,j}^{(l)}:={1\over m}\delta_{i,j}^{(l)}$ if j=0

说明：
1. $\delta_j^{(l)}$代表第l层第j个节点的“误差”
2. $\Delta$作为一个“误差累加器”，累计所有层误差以激活为权重的和
3. $D_{i,j}^{(l)}$是把$\Delta$经过正则化、取平均后的结果，表示损失函数对$\theta$的偏导数
$$D_{i,j}^{(l)} = {\partial J(\theta) \over \partial \theta_{i,j}^{(l)}} = {1\over m}\sum_{t=1}^m a_j^{(t)(l)}\delta_{i}^{(t)(l+1)}$$

### 直觉理解
当输出只有一个节点时，第t个样本的损失是
$$cost(t) = y^{(t)}log(h_{\theta}(x^{(t)}))+(1-y^{(t)})log(1-h_{\theta}(x^{(t)})) \\
\approx (h_{\theta}(x^{(t)})-y^{(t)})^2$$
$$\delta_j^{(l)} = {\partial\over \partial z_j^{(l)}}cost(t)$$

### 展开参数
把多个矩阵合成一个矩阵，为了方便调用优化函数
得到结果后展开

### 检查梯度
使用数值方法求一个梯度
$${\partial\over \partial \theta_j}J(\theta) \approx {J(\theta_1,...,\theta_j+\epsilon,...,\theta_n)-J(\theta_1,...,\theta_j-\epsilon,...,\theta_n) \over 2\epsilon}$$
这招用来验算，很慢

### 随机初始化
如果初始权重全为零，那么所有节点的值将会是相同的。
所以需要随机初始化。
一种初始化方法是：
$$\epsilon = {\sqrt{6} \over \sqrt{输出节点数+输出节点数}} \\
\theta^{(l)}=2\epsilon * (维度为输出节点数\times (输入节点数+1)的0-1随机矩阵) - \epsilon$$

### 设计与训练
首先选择一个网络构架

- 输入节点个数 = 训练样本特征维数
- 输出节点个数 = 分类数
- 每个隐藏层的节点个数： 越多越好，但要平衡计算成本
- 隐藏层个数： 默认1个，多于1个时每层节点个数相同

训练神经网络
1. 随机初始化权重
2. 通过forword propagation求得假设函数
3. 求损失函数
4. 通过back propagation求偏导
5. 使用梯度验证确保back propagation有效
6. 使用梯度下降或者内置优化函数通过权重最小化损失函数

forword propagation和back propagation在遍历样本的循环中进行。
